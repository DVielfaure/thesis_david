\Chapter{LITERATURE REVIEW}\label{sec:RevLitt}
The following chapter is an overview of the existing works found in the literature in relation with the research fields covered in the thesis. Attention will be brought to swarm programming languages and tools, information sharing in robot swarms, routing mechanisms, swarm exploration strategies, risk in swarm robotics and finally fault detection methods. 

\section{Swarm programming}
Drona \cite{desai2017drona} is a state-machine-based language providing decentralized motion planning for mobile drones. It can be run on the \ac{ROS} \cite{quigley2009ros} and offers real-time collision-free planning even with not perfectly synchronized clocks between robots. This is particularly useful as perfect clock synchronization is hard to achieve, especially in large and dynamic networks \cite{cao2021distributed}. However, Drona does not consider heterogeneous robot swarms.

Koord \cite{ghosh2020koord}, a new programming language for distributed robotic applications draws considerable attention to verification and validation of the distributed algorithm. Koord provides abstraction from the physical robot and enables easy verification of the code. Its modularity and hardware-independence means that each part of the algorithm can be easily tested and validated.

A programming language, specifically designed for large-scale robot swarms, has been proposed in \cite{pinciroliBuzz2016}. Buzz is an extensible programming language for heterogeneous robot swarms offering means of easily defining swarm behaviours both from a bottom-up and top-down perspective. Of course, Buzz respects the design principles of swarm robotic systems and self-organization is assured by the fully distributed run-time platform of the language. In addition to being an accessible swarm robotics programming tool, Buzz contains a large variety of the most common swarm behaviors, such as flocking, shape formation or barriers. It therefore enables fast prototyping and reduces the need for reprogramming these common swarm behaviors. The swarm-oriented programming language has also been integrated to \ac{ROS} \cite{st2017ros}.

ARGoS \cite{Pinciroli:SI2012} is a physics-based multi-robot simulator designed for large-scale experiments of heterogeneous swarms. ARGoS is time efficient thanks to its capability of running robots on separate threads. Experimental results demonstrate that simulation run-time increases linearly with the number of robots. The simulator is highly customizable where the simulated environment can be divided into subspaces with different physics engines. Moreover, Buzz and ARGoS can work together particularly well. Swarm behaviors implemented using the Buzz programming language can be easily tested in the ARGoS simulator, even with large robot swarms. Because of the physics incorporated in the simulator, the behaviors displayed in the ARGoS usually translate well to real-world scenarios. The combination of Buzz and ARGoS provides an accessible and fast swarm robotics development tool. 

\section{Information sharing}
Distributed sensing and information sharing is not trivial, especially considering the challenges of consistency and partial connectivity among the
robotic team~\cite{amigoni2017multirobot,otte2018emergent}. In social insects, pheromone trails are used to build a shared memory structure in the environment. For example, ants that communicate between each other by laying pheromones on the ground to guide peers towards food \cite{bonabeau1999swarm}. Such distributed memory mechanism is called stigmergy \cite{heylighen2016stigmergy1, heylighen2016stigmergy2} and allows agents to interact with each other without the need for direct communication and centralized control. 

The virtual stigmergy presented in \cite{pinciroliTuple2016} and implemented in the Buzz
programming language \cite{pinciroliBuzz2016} achieves consensus among
a group of robots using Conflict-free Replicated Data Types (CRDTs),
represented as key-value pairs shared and replicated among the swarm members. This sort of shared data structure is particularly relevant for belief maps, since it is easy to assign a unique key to each cell based on its location.  In the virtual stigmergy, data is shared on writing and reading the CRDT, with the
additional updates on read improving the robustness to temporary
disconnections and message drops. This solution differs from
distributed hash tables, which require a complete view of the system
at every point in time. Essentially, information updates are propagated throughout the swarm using the stigmergy whenever it is possible. In this sense, it offers high availability while settling for eventual data consistency. However, since the information is fully replicated among the agents of the system, storing large data items can prove challenging. SOUL \cite{varadharajan2020soul}, a file sharing protocol addresses the problem by storing information in the form of (key, blob) pairs. The blob’s metadata is fully replicated across the swarm, but the blobs are decomposed into datagrams stored on specific nodes of the system. Other distributed data storage approaches such as SwarmMesh \cite{majcherczykSwarmmesh2020} store data in different
locations based on a fitness function instead of replicating them on
all robots. This allows the storage of more data with less
communication, but robots are less likely to have access to the latest
values.

Belief maps are a simple yet powerful tool for robotic exploration.
They render a continuous surface into a discrete set of cells which is especially useful for engineering problems. It makes possible highlighting specific regions of the environment and allows the designer of the algorithm to tune the level of precision at which the environment is represented. A finer granularity will provide a more accurate representation of the environment and as a result should offer more precision. However, finer granularity usually results in higher computational costs. The cells of the belief map are used to store information about the corresponding surface they represent. They generally store a probability that indicates the confidence level to which the cell is believed to contain a specific feature. Belief maps are a generalization of occupancy maps: instead of storing only one
bit per cell to indicate the presence of an obstacle/danger, they
store obstacle/danger likelihood and offer significant improvements
for exploration \cite{stachnissMappingExplorationMobile2003}. In the
field of multi-robot exploration, early techniques leveraging belief
maps date back as far as twenty years
\cite{kobayashiSharingExploringInformation2002,kobayashiDeterminationExplorationTarget2003},
but they rely on a fixed grid size and are tested only with two
robots. More recent works also leverage belief maps for multi-robot
exploration. For example,
in~\cite{indelmanCooperativeMultirobotBelief2018}, the robots consider
both the current beliefs and the expected beliefs from future
observations to coordinate their exploration. Grid maps and belief
maps are also widely used to train deep reinforcement learning
exploration policies
\cite{hanGridWiseControlMultiAgent,panovGridPathPlanning2018}, but they often rely on a trial and error process which may select actions leading to failures \cite{garciaSafeExplorationState2012,andersenSafeReinforcementlearningIndustrial2020}.


\section{Routing}
Data storage and routing remains a challenge for robot swarms as the amount of collected data items only increases with the number of robots in the system. Because robot swarms typically suffer from bad connectivity \cite{amigoni2017multirobot}, sending collected data items directly to external storage might not always be possible. Robots often need to store locally the data items until a path towards permanent storage becomes available. Because robots usually have limited communication range, the data items collected during the mission may need to be routed through multiple robots before reaching the external storage infrastructure. The multi-robot system becomes a temporary storage infrastructure and deciding where to store and send the data items become essential.
One of the most popular approaches for routing data items in swarm robotics is to use a gradient-based routing scheme \cite{faruque2005analysis,draves2004comparison,watteyne2009implementation}. In gradient-based routing, a scalar value (also called height) is assigned to each of the nodes of the system. The value is based on a metric that evaluates how fit this node is to be used in the routing scheme. For example, in a simple hop count based routing protocol, the scalar value would increase with the distance with the base station. Then, the nodes through which data items are routed are simply chosen by using the neighbor that displays the lowest height. Probably the most used metric for defining the height of the nodes is hop count \cite{kuruvila2005hop,zhang2014efficient,al2019efficient} as it allows fast and efficient routing of data items. However, other cost functions for determining the height of the nodes are possible.

Recently, many routing strategies have been developed for vehicular ad hoc networks (VANETs). In \cite{tripp2019survey}, a survey on routing protocols for such networks is presented. The survey focuses on routing protocols that use several metrics as they have shown to be effective in dynamic networks. Specifically, protocols based on the nodes' geographic positions are the most adequate as the dynamicity of the network is directly included in the routing scheme \cite{boussoufa2018geographic, fussler2002comparison}.

Other notable routing mechanisms take inspiration from nature. In \cite{li2011slime,jiang2018toward}, path growth routing protocols based on slime molds are presented. In \cite{jiang2018effective,liao2008data}, ant colony optimization-based routing algorithms are presented. The ant colony optimization algorithm tries to reproduce the behavior of ants in their search of the nearest food sources \cite{dorigo1996ant}. In the process, ants lay pheromones on the ground. Hence, the shorter the path between the nest and the food source, the more frequent it will be traversed by ants, thus increasing the pheromone intensity. Ants then follow paths with high pheromone intensities. In \cite{liao2008data} existing approaches are investigated and a hybrid routing mechanism that combines ant colony optimization and hop count into one routing scheme is proposed. However, these approaches are tailored for static topologies and are ill suited for dynamic robotic networks.


\section{Swarm exploration strategies}
Many distributed exploration strategies maximizing the amount of
terrain coverage have been proposed. The first approaches to stand out
in this regard are Voronoi-based coverage control
techniques~\cite{arslan2016voronoi,luo2019voronoi}.
Specific to \cite{luo2019voronoi}, connectivity awareness is included in the Voronoi-based coverage to guarantee that a minimum of connectivity is maintained between the robots of the system. 

A second method covers time-varying domains, in which points in the
covered region can become more or less interesting to explore,
therefore prompting a change in the coverage function
\cite{santos2019decentralized,xu2019multi}. 

Another method to optimize coverage is Frontier-Based Exploration (FBE)
\cite{yamauchi1998frontier} of which many variations have been
developed, such as those based on Particle Swarm Optimization
\cite{wang2011frontier} or the Wavefront Frontier Detector
\cite{topiwala2018frontier}. FBE's key principle is to assign one of three states to the cells constituting the environment:

\begin{itemize}
\item \textbf{Explored:} cells that have already been explored
\item \textbf{Frontier:} cells between explored and unexplored space
\item \textbf{Unexplored:} cells that remain uncovered
\end{itemize}

Using the state of the cells, robots coordinate to explore the regions at the frontier, thereby expanding them and eventually achieving full map coverage.

Other interesting coverage techniques take inspiration from insects and leverage virtual pheromones for encouraging or discouraging robots to explore specific regions of the environment. For example, in ant foraging, negative feedback in the form of pheromones is used to discourage ants from using unprofitable paths \cite{robinson2005no}. In \cite{hunt2019testing}, pheromones are employed to dissuade robots of covering already covered areas of the environment. Authors show that using repellent pheromones can effectively encourage a rapid dispersal of robots even in large swarm systems. In Phormica \cite{salman2020phormica}, robots start exploring the environment in a random fashion. By projecting \ac{UV} light on the ground, the robots are able to convey information to their peers and let them know that the region has already been covered. When the artificial pheromone is detected, robots can change their course of action and move towards unaltered regions of the environment.

However, none of these aforementioned coverage strategies take
risk into account. In
\cite{dames2012decentralized,schwagerMultirobotControlPolicy2017}
an exploration algorithm that maximizes information gain in the presence of unknown hazards is presented. Unfortunately, this optimal algorithm has a
very high computational complexity and could benefit from approximations.



\section{Risk in swarm robotics}
The importance of risk management has dramatically increased over the last few years. Robotic systems are being used more widely and as a result, risk management becomes essential as to not endanger the system itself and the objects and beings in its immediate environment. The importance of enabling distributed situational awareness in robot swarms is raised in \cite{jones2020distributed}. In \cite{hunt2020checklist}, a checklist for robot swarms to be safe for the public, the environment and for itself is presented. The authors draw attention to the lack of proper systematic swarm safety assessment mechanism and propose a checklist, in the form of ten questions, that should be answered when designing such systems. They consider that swarm safety is larger than simply analyzing failure modes. Questions related to ethics, legality, accountability, security are also listed and as a result, should provide a more thorough consideration of all socio-technical risks robot swarms face. 

In \cite{higgins2009threats}, the security challenges of robots swarms are presented. Security is defined as the state of being protected from risks originating from hostile and malicious intentions. The security of new technologies is usually not included in its design process, typically it is with the rise of the technology that security concerns appear. To prevent any unwanted consequences, the paper presents the security challenges robot swarms will face in an attempt of including this component in the design process of the new technology. However, only security threats are considered and risk, as something not malicious, is not examined.

Several path planners based on Markov Decision Processes \cite{undurti2010online,thiebaux2016rao,xiao2020robot} take into account risk and have useful definitions of it. In \cite{xiao2020robot}, risk is categorized into three different groups:

\begin{itemize}
    \item \textbf{Locale-Dependent:} Risk elements not depending on history. The risks associated with this category are usually location-based, in other words, it is the position of the robot in the environment that determines the level of risk. 
    \item \textbf{Action-Dependent:} Risk elements depending on close history, specifically changes of states. For example, the risk associated with an aggressive turn is tied to the last states of the robot.   
    \item \textbf{Traverse-Dependent:} Risk elements depending on the entire history of the robot. Risks associated with this category are tied to all the states traversed by the robot. For example, risk associated with low battery levels are included in the category.  
\end{itemize}

In \cite{hakobyan2019risk}, a risk-aware motion planning and decision-making mechanism is presented. They automatically adjust the "conservativeness" of the motion-planner based on the risk a robot faces. The paper uses a conditional value-at-risk method used in finance to estimate the risk of an investment. They use a safety risk measure from \cite{samuelson2018safety} onto which they apply the conditional value-at-risk method to achieve safe motion planning and control. Unfortunately, risk only includes collisions and does not translate to other types of hazard robots could face. Value-at-risk strategies for robot swarms in hazardous environments were also studied in \cite{hunt2021value}. Again, the method of value-at-risk allows quantifying the foreseen loses over a period of time where the environment contains potentially damaging radiation sources. In detail, agents of the swarm calculate the value-at-risk at every time step and share it with their neighbors when their value-at-risk limit is exceeded. The information helps team members avoid dangerous locations of the environment and overall decreases exposure to risk. Some shortcomings of the method include the determination of the value-at-risk limit and the lack of instantaneous responsiveness of the method as it relies on past observations. Overall, the value-at-risk methods show that financial risk management techniques are an interesting avenue for risk awareness in swarm robotics.

Another interesting idea for risk awareness is proposed in \cite{ono2008efficient,vitus2011feedback}, where a "risk budget" is allocated to their agents, allowing them to optimize a balance between risk and reward to guide robots. However, these systems assume knowledge of the global state of the environment, which is unavailable when exploring unknown environments. Furthermore, most are only applied to single-robot systems. 

In SPIDER \cite{hunt2020spider}, multiple agents are tasked with chain formation in dangerous environments. They adapt to varying levels of risk to be resilient to significant failures and member losses in order to maximize information gathering. They introduce a level of "boldness" which represent the risk appetite of the agents of the swarm. This risk appetite is modulated by the connectivity of the agent, specifically its frequency of interactions of neighbors. When an agent is well connected, its risk appetite grows and, as a result, should be encouraged to explore new parts of the environment. On the other hand, an agent that is isolated and far from any other members of the swarm will increasingly display a shy behavior and go back to safer areas of the environment. SPIDER effectively allows a swarm to trade off the benefits of information gain versus robot failures. However, the problem is only studied for a chain formation scenario and risk is only a measure of how well connected is an agent.  






\section{Fault detection}

A taxonomy of the faults affecting robotic systems is presented in \cite{khalastchi2018fault}. Hardware faults affect physical components of the system. They compromise the sensing and acting abilities of the robots. Software faults affect the behaviour of the robots and are caused by faulty algorithms and/or faulty implementations. Interaction faults affect the dynamics of the robotic system and are caused by exogenous events. Hopefully, fault detection methods have been developed to mitigate the presence of faults in robotic systems. They are divided into three big families: data-driven; model-based; knowledge-based \cite{khalastchi2018fault}. 

\begin{itemize}
\item \textbf{Data-driven} approaches use sensor data and compare it to known faults, to past normal/abnormal behaviors or to the behavior of neighbour agents. Using statistical tools, data-driven approaches compute the deviation of the data and classify it as normal/abnormal depending on the extent of this deviation.
\item \textbf{Model-based} approaches use an \textit{a priori} explicit model of the system to identify faults. The model is a set of analytical equations or logical formulas. When an irregularity is identified between what is observed and the theoretical model, a fault is presumed. The main drawback of these approaches is the work needed in constructing the theoretical model. 
\item \textbf{Knowledge-based} approaches are similar to the way a human would perform fault detection. A fault is quickly associated with its cause. Faults are typically represented in a tree structure where the fault can be linked backwards to where it originated. It is particularly useful for fault isolation.
\end{itemize}

Outlier detection methodologies have been widely used for identifying anomalies that could be the result of a fault. A definition of “outlier” was proposed by Grubbs (Grubbs, 1969): 

\begin{center}
\textit{An outlying observation, or outlier, is one that appears to deviate
markedly from other members of the sample in which it occurs.}
\end{center} 

Outlier detection is used for classifying data as normal or abnormal. Applying it to fault detection is to suppose that the abnormality observed is the result of a fault. Outlier detection is part of the data-driven family of the fault detection approaches. From \cite{hodge2004survey}, outlier detection can be divided into 3 types: 

\begin{itemize}
    \item \textbf{Unsupervised clustering:} The outlier detection does not need any prior knowledge of the data. Using the distribution of the data, the most isolated points are classified as outliers.
    \item \textbf{Supervised classification:} The outlier detection needs a prior knowledge of normality and abnormality. Using pre-labelled normal/abnormal data, new incoming data can be classified in either of these classes based on its distance. 
    \item \textbf{Semi-supervised recognition:} The outlier detection needs prior knowledge of normality. Using pre-labelled normal data, new incoming data can be classified as normal or abnormal. It resembles type 2 outlier detection but without the need for labelled abnormal data.
\end{itemize}


Additionally, two strategies can be used for fault detection: endogenous fault detection and exogenous fault detection \cite{christensen2008fault, lau2012error, Miller2021modern}. Endogenous fault detection refers to the actions taken by an individual to perform fault detection on itself and on its own. While this approach is well suited for single robots, in the context of multi-robot systems, opting for this approach does not take advantage of the multiple entities close to one another forming the swarm. On the other hand, exogenous fault detection refers to the actions taken by neighbour robots on a central entity. This approach relies on the observations of multiple robots and is, as a result, better suited for swarm robotics. Exogenous data-driven fault detection methods are the best suited for robot swarms. Exogenous strategies leverage the power of swarms through collaboration of all team members. Data-driven methods allows the detection of previously unseen faults and, as a result, enable the deployment of the swarm in unknown and dynamic environments.


A fault detection method inspired by the human immune system has been proposed in \cite{ tarapore2015err, tarapore2017generic}. The method uses the mathematical formulation of the cross-regulation model to distinguish between normal and abnormal behaviors in a swarm of robots. Abnormality is the indication of a fault, the abnormality being the result of it. The fault detection method is divided into three main phases: (i) observing the behaviors of the agents; (ii) based on the observed behaviors, perform anomaly detection using the cross-regulation model; (iii) voting to decide if the agent’s behavior is normal or abnormal. The first phase of the method (i) uses feature vectors to characterize the behavior of an agent. The feature vector contains six features $(F_1, F_2, F_3, F_4, F_5, F_6)$, each indicating the presence $(Fi=1)$ or absence $(Fi=0)$ of a particular behavior. Then, the method uses these informative binary feature vectors for running the cross-regulation model (ii). In this fault detection method, if at the end of the cross-regulation cycle, the \ac{APC} (modelled as a feature vector) is considered a foreign pathogen, an abnormality is inferred. Finally, in the last phase of the methods (iii) the agents consolidate their individual decisions and vote on the normal/faulty behavior of the neighboring agents. Agents that receive more than five “foreign pathogen” votes are considered faulty. The method was tested on a swarm of seven physical robots and results showed that the method was able to reliably identify the faults injected in the system \cite{tarapore2019fault}. The efficiency of the method actually surpassed the ones of other traditional outlier detection methods, namely \ac{KNN} and  \ac{LOF}. However, for both \ac{KNN} and \ac{LOF} binary feature vectors were used, however, these methods could benefit from using non-binary feature vectors \cite{tarapore2019fault}. The advantage of this method is that it does not require any prior knowledge of the fault to identify it. Indeed, being an outlier detection method, the presence of faults is only inferred on the presence of an abnormal behavior. This is particularly important in swarm of robots where it is very difficult to know in advance the potential fault that the system will encounter. It is also easier to implement. However, because the method identifies anomalies and not faults, it is very hard to perform diagnosis and recovery procedures. It is difficult to assess the correct plan of action when the nature of the problem remains unknown. In \cite{okeefe2018adaptive}, the problem is solved by introducing a fault diagnosis layer after the fault detection one. 

The \ac{LOF} \cite{breunig2000lof} is a measure of how much a data point can be considered an outlier. The \ac{LOF} represents the density of a point compared to the density of its neighbors. A \ac{LOF} of around 1 means that the point is not an outlier. A \ac{LOF} much higher than 1 indicates that the density of the data point is smaller the density of the neighboring points. The data point is isolated and is, as a result, most likely to be an outlier. In summary, the higher the \ac{LOF}, the most likely the data point is an outlier. 

A neural network fault detection approach was proposed in \cite{christensen2008faultDetection}. The approach assumes that the occurrence of a fault in the robotic system will cause the flow of sensory data to change. By monitoring it and feeding it to a neural network, the method is capable of detecting faults in the system with manageable latency using s-bots robots \cite{mondada2005cooperation}. For every control cycle, sensory data is given as an input to the neural network which, in turn, outputs the state of the system (0 or 1), with 0 corresponding to not faulty and 1 to faulty. Again, this falls into the anomaly detection category and more specifically under the data-driven group. One of the drawback of the method is the need for training runs to train the neural network.

In recent years, deep learning anomaly detection methods have been developed and have shown to generally outperform the traditional anomaly detection methods \cite{pang2021deep}. They are especially effective when working with complex and large data flows where traditional methods tend to struggle.



